import time

import requests

import app.api.routes as routes
from app.core.config import LIST_API_BASE, HEADERS
from .comic_service import crawl_comic


def fetch_page_data(page: int):
    url = f"{LIST_API_BASE}?page={page}"
    for _ in range(3):
        try:
            res = requests.get(url, headers=HEADERS, timeout=15)
            if res.status_code == 429:
                time.sleep(8)
                continue
            data = res.json()
            if data.get("status") == "success":
                return data
        except Exception:
            time.sleep(3)
    return None


def crawl_all():
    page = 1
    while True:
        print(f"\nğŸŒ Äang crawl trang {page}")
        data = fetch_page_data(page)
        if not data:
            print(f"ğŸ›‘ Háº¿t dá»¯ liá»‡u hoáº·c lá»—i á»Ÿ trang {page}. Dá»«ng láº¡i.")
            routes.checkCrawl = True
            break

        items = data["data"].get("items", [])
        images = data["data"]["seoOnPage"].get("og_image", [])
        if not items:
            print("ğŸ›‘ KhÃ´ng cÃ²n truyá»‡n nÃ o â€” dá»«ng crawl.")
            routes.checkCrawl = True
            break

        for i, item in enumerate(items):
            slug = item.get("slug")
            image_for_item = images[i] if i < len(images) else None

            ok = crawl_comic(slug, image_from_list=image_for_item)
            if not ok:
                print(f"â›” Gáº·p truyá»‡n cÅ© hoáº·c lá»—i táº¡i slug={slug} â€” dá»«ng toÃ n bá»™ crawl_all.")
                routes.checkCrawl = True
                return

            time.sleep(0.2)

        page += 1

    print(f"\nâœ… HoÃ n táº¥t crawl toÃ n bá»™.")
    routes.checkCrawl = True
